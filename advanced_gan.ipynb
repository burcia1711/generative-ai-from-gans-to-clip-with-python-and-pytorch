{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEpXvJpNAhVU"
      },
      "source": [
        "**mode collapse:** generator stuck in a single mode (peak of the distribution)\n",
        "\n",
        "**loss value meaning:** not a good correlation between the loss value and the quality of the generated results\n",
        "\n",
        "**bce loss problems -** flat gradients\n",
        "\n",
        "we'll use **the wasserstein loss** instead of binary cross entropy.\n",
        "\n",
        "**earth mover distance:** the effort needed to make both real and generated distributions equal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pqS17Ro9ZPN"
      },
      "outputs": [],
      "source": [
        "# coding an advanced GAN from scratch in python + pytorch\n",
        "# importing the libraries\n",
        "\n",
        "import torch, torchvision, os, PIL, pdb # pytorch and torchvision are for datasets and image transforms, os if for file handling, pil is for image processing, and pdb if for debugging\n",
        "from torch import nn # neural network module\n",
        "from torch.utils.data import Dataset # dataset class and data loader to create custom datasets and load them in batches\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms # transforms module from torchvision to apply image preprocessing\n",
        "from torchvision.utils import make_grid # function to create a grid of images for visualisation\n",
        "from tqdm.auto import tqdm # progress bar tool for loops\n",
        "import matplotlib.pyplot as plt # for plotting images and graphs\n",
        "import numpy as np # for numerical operations\n",
        "from PIL import Image # image module from pillow for image loading and handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "My-ARoryrNFP"
      },
      "outputs": [],
      "source": [
        "def show(tensor, num = 25, wandbactive = 0, name = ''):\n",
        "  data = tensor.detach().cpu()   # detach tensor from computation graph and move it to cpu\n",
        "  grid = make_grid(data[:num], nrow = 5).permute(1, 2, 0)   # create a grid of images (5 per row), rearrange axes for plotting\n",
        "\n",
        "  # optional: log images to wandb if enabled\n",
        "  if (wandbactive == 1):\n",
        "    wandb.log({name: wandb.Image(grid.numpy().clip(0,1))})\n",
        "\n",
        "  plt.imshow(grid.clip(0, 1))\n",
        "  plt.show()\n",
        "\n",
        "#hyperparameters and general parameters\n",
        "n_epochs = 100000 # number of training iterations\n",
        "batch_size = 128 # number of images per batch\n",
        "lr = 1e-4 # learning rate for optimiser\n",
        "z_dim = 200 # size of the noise vector for generator\n",
        "device = 'cuda' # GPU for training\n",
        "\n",
        "cur_step = 0 # current step in training\n",
        "crit_cycles = 5 # number of critic updates per generator update (used in wgan)\n",
        "gen_losses = [] # to track generator loss over time\n",
        "crit_losses = [] # to track critic/discriminator loss over time\n",
        "show_step = 35 # how often to display generated image\n",
        "save_step = 35 # how often to display model check points\n",
        "\n",
        "wandbact = 1 # yes we want to track stats through weights and biases, optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ztz5VHQ_XU28"
      },
      "outputs": [],
      "source": [
        "# optional\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login(key = 'f689bd43ce9af73cf9f2e49c99523a9449db9c5e' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SprUwf8albim"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "experiment_name = wandb.util.generate_id() # generate a unique id for the experiment\n",
        "\n",
        "myrun = wandb.init( # initialise a wandb run\n",
        "    project = 'wgan', # name of the project: wassersteing gan\n",
        "    group = experiment_name, # group name used to group related runs\n",
        "    config = { # configuration dictionary\n",
        "        'optimizer': 'sgd', # use stochastic gradient descent\n",
        "        'model': 'wgan gp', # model type is wasserstein gan with gradient penalty\n",
        "        'epoch': 1000, # traning epochs\n",
        "        'batch_size': 128 # samples per batch\n",
        "    }\n",
        ")\n",
        "\n",
        "config = wandb.config # store wand config for early access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBZP3ieMnN_e"
      },
      "outputs": [],
      "source": [
        "print(experiment_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzxwR-qYqLvh"
      },
      "outputs": [],
      "source": [
        "# generator model\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, z_dim = 64, d_dim = 16):\n",
        "    super(Generator, self).__init__()\n",
        "    self.z_dim = z_dim\n",
        "\n",
        "    self.gen = nn.Sequential(\n",
        "        ## ConvTranspose2d: in_channels, out_channels, kernel_size, stride = 1, padding = 0\n",
        "        ## calculating new width and height: ((n-1) * stride) - (2 * padding) + ks\n",
        "        ## n: width or height\n",
        "        ## ks: kernel size\n",
        "        nn.ConvTranspose2d (z_dim, d_dim * 32, 4, 1, 0 ), ## we begin with a 1x1 image with z_dim number of channels (200) --> 4x4 (ch: 200 -> 512)\n",
        "        nn.BatchNorm2d(d_dim * 32), # normalise values\n",
        "        nn.ReLU(inplace = True), # add non-linearity\n",
        "\n",
        "        nn.ConvTranspose2d (d_dim * 32, d_dim * 16, 4, 2, 1), ## 8x8 (ch: 512 -> 256)\n",
        "        nn.BatchNorm2d(d_dim * 16), # normalise values\n",
        "        nn.ReLU(inplace = True), # add non-linearity\n",
        "\n",
        "        nn.ConvTranspose2d (d_dim * 16, d_dim * 8, 4, 2, 1), ## 16x16 (ch: 256 -> 128) because ((n-1) * stride) - (2 * padding) + ks = ((8-1) * 2) - (2 * 1) + 4) = 16\n",
        "        nn.BatchNorm2d(d_dim * 8), # normalise values\n",
        "        nn.ReLU(inplace = True), # add non-linearity\n",
        "\n",
        "        nn.ConvTranspose2d (d_dim * 8, d_dim * 4, 4, 2, 1), ## 32x32 (ch: 128 -> 64)\n",
        "        nn.BatchNorm2d(d_dim * 4), # normalise values\n",
        "        nn.ReLU(inplace = True), # add non-linearity\n",
        "\n",
        "        nn.ConvTranspose2d (d_dim * 4, d_dim * 2, 4, 2, 1), ## 64x64 (ch: 64 -> 32)\n",
        "        nn.BatchNorm2d(d_dim * 2), # normalise values\n",
        "        nn.ReLU(inplace = True), # add non-linearity\n",
        "\n",
        "        nn.ConvTranspose2d (d_dim * 2, 3, 4, 2, 1), ## 128x128 (ch: 32 -> 3)\n",
        "        nn.Tanh() # add non-linearity, produce result in the range from -1 to 1\n",
        "\n",
        "    )\n",
        "\n",
        "  def forward(self, noise):\n",
        "    x = noise.view(len(noise), self.z_dim, 1, 1) # reshape noise to (batch_size, z_dim, 1, 1) ---> 128 x 200 x 1 x 1\n",
        "    return self.gen(x)\n",
        "\n",
        "\n",
        "def gen_noise(num, z_dim, device = 'cuda'):\n",
        "  return torch.randn(num, z_dim, device = device) # set of noise vectors of 128 x 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmTUkqY44gNE"
      },
      "outputs": [],
      "source": [
        "# critic model\n",
        "\n",
        "class Critic (nn.Module):\n",
        "  def __init__(self, d_dim = 16):\n",
        "    super(Critic, self).__init__()\n",
        "\n",
        "    self.crit = nn.Sequential(\n",
        "        ## Conv2d: in_channels, out_channels, kernel_size, stride = 1, padding = 0\n",
        "        ## calculating new width and height: ((n + (2 * padding) - ks) // stride) + 1\n",
        "        nn.Conv2d(3, d_dim, 4, 2, 1), ## (n + (2 * padding) - ks) // stride + 1 = ((128 + (2 * 1) - 4)//2) + 1 = 64x64 (ch: 3 -> 16)\n",
        "        nn.InstanceNorm2d(d_dim), # normalise values by instance (works well)\n",
        "        nn.LeakyReLU(0.2), # add non-linearity\n",
        "\n",
        "        nn.Conv2d(d_dim, d_dim * 2, 4, 2, 1), ## 32x32 (ch: 16 -> 32)\n",
        "        nn.InstanceNorm2d(d_dim * 2), # normalise values\n",
        "        nn.LeakyReLU(0.2), # add non-linearity\n",
        "\n",
        "        nn.Conv2d(d_dim * 2, d_dim * 4, 4, 2, 1), ## 16x16 (ch: 32 -> 64)\n",
        "        nn.InstanceNorm2d(d_dim * 4), # normalise values\n",
        "        nn.LeakyReLU(0.2), # add non-linearity\n",
        "\n",
        "        nn.Conv2d(d_dim * 4, d_dim * 8, 4, 2, 1), ## 8x8 (ch: 64 -> 128)\n",
        "        nn.InstanceNorm2d(d_dim * 8), # normalise values\n",
        "        nn.LeakyReLU(0.2), # add non-linearity\n",
        "\n",
        "        nn.Conv2d(d_dim * 8, d_dim * 16, 4, 2, 1), ## 4x4 (ch: 128 -> 256)\n",
        "        nn.InstanceNorm2d(d_dim * 16), # normalise values\n",
        "        nn.LeakyReLU(0.2), # add non-linearity\n",
        "\n",
        "        nn.Conv2d(d_dim * 16, 1, 4, 1, 0), ## 1x1 (ch: 256 -> 1) # this will tell if the image is real or fake\n",
        "\n",
        "    )\n",
        "\n",
        "  def forward(self, image):\n",
        "    # image: 128 x 3 x 128 x 128\n",
        "    crit_pred = self.crit(image) # pass image through critic, 128 x 1 x 1 x 1\n",
        "    return crit_pred.view(len(crit_pred), -1) # reshape output to (batch_size, 1): 128 x 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19XEkUtD2ntV"
      },
      "outputs": [],
      "source": [
        "# optional way of initialising the parameters\n",
        "def init_weights(m):\n",
        "  if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "    torch.nn.init.normal_(m.weight, 0.0, 0.02) # initialise weights with a normal distribution with 0 mean and 0.02 variance\n",
        "    torch.nn.init.constant_(m.bias, 0) # set all biases to zero\n",
        "\n",
        "  if isinstance(m, nn.BatchNorm2d):\n",
        "    torch.nn.init.normal_(m.weight, 0.0, 0.02) # initialise batchnorm weights with a normal distribution\n",
        "    torch.nn.init.constant_(m.bias, 0) # set all biases to zero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWE40U-X-wit"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "import gdown, zipfile # import libraries for downloading and extracting files\n",
        "\n",
        "URL = 'https://dl.dropboxusercontent.com/scl/fi/vltmt8hlgdf9mv9kn7d0b/img_align_celeba.zip?rlkey=tacwpkr8d9bjpctdftjg3b00a' # current url of the dataset\n",
        "path = 'data/celeba' # target directory to save the dataset\n",
        "download_path = f'{path}/img_align_celeba.zip' # full path for the downloaded zip file\n",
        "\n",
        "if not os.path.exists(path): # check if the directory exists\n",
        "  os.makedirs(path, exist_ok = True) # create the directory if it doesn't exist\n",
        "\n",
        "gdown.download(URL, download_path, quiet = False) # download the dataset using gdown\n",
        "\n",
        "with zipfile.ZipFile(download_path, 'r') as ziphandler: # open the zip file\n",
        "  ziphandler.extractall(path) # extract all contents to the target directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_LMP5hoMr4H"
      },
      "outputs": [],
      "source": [
        "# dataset, dataLoader; declare gen, crit, test dataset\n",
        "\n",
        "class Dataset(Dataset): # custom dataset class\n",
        "  def __init__(self, path, size = 128, lim = 10000): # initialise with image path, resize size, and limit\n",
        "    self.sizes = [size, size] # target size for resizing images\n",
        "    items, labels = [], []\n",
        "\n",
        "    for data in os.listdir(path)[:lim]: # loop through limited number of image files\n",
        "      # path: '.data/celeba/img_align_celeba'\n",
        "      # data: '114568.jpg'\n",
        "      item = os.path.join(path, data) # full path to image\n",
        "      items.append(item) # add image path to list\n",
        "      labels.append(data) # use filename as label\n",
        "    self.items = items\n",
        "    self.labels = labels\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.items) # return total number of items\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    data = PIL.Image.open(self.items[idx]).convert('RGB') # open image and convert to rgb\n",
        "    data = np.asarray(torchvision.transforms.Resize(self.sizes)(data)) # resize image to 128x128x3\n",
        "    data = np.transpose(data, (2, 0, 1)).astype(np.float32, copy=False) # transpose to channel first format: 3x128x128 and convert to float32\n",
        "    data = torch.from_numpy(data).div(255) # convert to tensor and normalise to [0, 1]\n",
        "    return data, self.labels[idx] # return preprocessed image and its label\n",
        "\n",
        "# dataset\n",
        "data_path = './data/celeba/img_align_celeba' # dataset folder path\n",
        "ds = Dataset(data_path, size = 128, lim = 10000) # create dataset instance\n",
        "\n",
        "# dataloader\n",
        "dataloader = DataLoader(ds, batch_size = batch_size, shuffle = True) # create dataloader with shuffling\n",
        "\n",
        "# models\n",
        "gen = Generator(z_dim).to(device) # create generator model and move to device\n",
        "crit = Critic().to(device) # create critic model and move to device\n",
        "\n",
        "# optimisers\n",
        "gen_opt = torch.optim.Adam(gen.parameters(), lr = lr, betas = (0.5, 0.9)) # optimiser for generator\n",
        "crit_opt = torch.optim.Adam(crit.parameters(), lr = lr, betas = (0.5, 0.9)) # optimiser for critic\n",
        "\n",
        "# initialisations\n",
        "## gen = gen.apply(init_weights) # apply initialisation to the generator\n",
        "## crit = crit.apply(init_weights) # apply initialisation to the critic\n",
        "\n",
        "# wandb optional\n",
        "if (wandbact == 1):\n",
        "  wandb.watch(gen, log_freq = 100) # track generator parameters and gradients every 100 steps\n",
        "  wandb.watch(crit, log_freq = 100) # track critic parameters and gradients every 100 steps\n",
        "\n",
        "x, y = next(iter(dataloader)) # fetch one batch from the dataloader\n",
        "show(x) # display the batch of images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3JY5nhbWvwH"
      },
      "outputs": [],
      "source": [
        "# gradient penalty calculation\n",
        "\n",
        "def get_gp(real, fake, crit, alpha, gamma = 10):\n",
        "  mix_images = real * alpha + fake * (1-alpha) # interpolate between real and fake images (shape: 128 x 3 x 128 x 128)\n",
        "  mix_scores = crit(mix_images) # pass mixed images through critic to get scores (shape: 128 x 1)\n",
        "\n",
        "  gradient = torch.autograd.grad( # calculate gradient of mix_scores with respect to mix_images\n",
        "    inputs = mix_images,\n",
        "    outputs = mix_scores,\n",
        "    grad_outputs = torch.ones_like(mix_scores), # set gradient output to 1\n",
        "    create_graph = True, # retain computation graph for higher-order derivatives\n",
        "    retain_graph = True\n",
        "  )[0] # extract gradient tensor (shape: 128 x 3 x 128 x 128)\n",
        "\n",
        "  gradient = gradient.view(len(gradient), -1) # flatten each gradient to a vector (shape: 128 x 49152)\n",
        "  gradient_norm = gradient.norm(2, dim = 1) # calculate L2 norm of each gradient vector (shape: 128)\n",
        "\n",
        "  gp = gamma * torch.mean((gradient_norm - 1) ** 2) # compute gradient penalty (should be close to 0 if norm ≈ 1)\n",
        "  return gp # return scalar gradient penalty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6py50VBZPSN"
      },
      "outputs": [],
      "source": [
        "# save and load checkpoints\n",
        "\n",
        "root_path='./data/' # directory to store checkpoint files\n",
        "\n",
        "def save_checkpoint(name):\n",
        "  torch.save({ # save generator's training state\n",
        "      'epoch': epoch, # current epoch\n",
        "      'model_state_dict': gen.state_dict(), # generator weights\n",
        "      'optimiser_state_dict': gen_opt.state_dict() # generator optimiser state\n",
        "  }, f\"{root_path}G-{name}.pkl\")\n",
        "\n",
        "  torch.save({ # save critic's training state\n",
        "      'epoch': epoch, # current epoch\n",
        "      'model_state_dict': crit.state_dict(), # critic weights\n",
        "      'optimiser_state_dict': crit_opt.state_dict()  # critic optimiser state\n",
        "  }, f\"{root_path}C-{name}.pkl\")\n",
        "\n",
        "  print(\"Saved checkpoint\") # confirm saving\n",
        "\n",
        "def load_checkpoint(name):\n",
        "  checkpoint = torch.load(f\"{root_path}G-{name}.pkl\") # load generator checkpoint\n",
        "  gen.load_state_dict(checkpoint['model_state_dict']) # load generator weights\n",
        "  gen_opt.load_state_dict(checkpoint['optimiser_state_dict']) # load generator optimiser state\n",
        "\n",
        "  checkpoint = torch.load(f\"{root_path}C-{name}.pkl\") # load critic checkpoint\n",
        "  crit.load_state_dict(checkpoint['model_state_dict']) # load critic weights\n",
        "  crit_opt.load_state_dict(checkpoint['optimiser_state_dict']) # load critic optimiser state\n",
        "\n",
        "  print(\"Loaded checkpoint\")  # confirm loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmQhhgCrdv4f"
      },
      "outputs": [],
      "source": [
        "# some tests\n",
        "\n",
        "# epoch = 1\n",
        "# save_checkpoint('test') # save test checkpoint\n",
        "# load_checkpoint('test') # load test checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3ACSz1gd2YA"
      },
      "outputs": [],
      "source": [
        "# training loop\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  for real, _ in tqdm(dataloader): # iterate over each batch\n",
        "    cur_batch_size = len(real) # get current batch size (typically 128)\n",
        "    real = real.to(device) # move real images to GPU\n",
        "\n",
        "    ### critic ###\n",
        "    mean_crit_loss = 0\n",
        "    for _ in range(crit_cycles): # update critic multiple times per generator update\n",
        "      crit_opt.zero_grad() # reset gradients\n",
        "\n",
        "      noise = gen_noise(cur_batch_size, z_dim) # generate random noise\n",
        "      fake = gen(noise) # generate fake images using generator\n",
        "      crit_fake_pred = crit(fake.detach()) ## critic prediction on fake (detach to avoid generator gradients)\n",
        "      crit_real_pred = crit(real) # critic prediction on real\n",
        "\n",
        "      alpha = torch.rand(len(real), 1, 1, 1, device = device, requires_grad = True) # random interpolation factor\n",
        "      gp = get_gp(real, fake.detach(), crit, alpha) # calculate gradient penalty\n",
        "\n",
        "      crit_loss = torch.mean(crit_fake_pred) - torch.mean(crit_real_pred) + gp # WGAN-GP loss\n",
        "\n",
        "      mean_crit_loss += crit_loss.item() / crit_cycles # accumulate average critic loss\n",
        "\n",
        "      crit_loss.backward(retain_graph = True) # backpropogate\n",
        "      crit_opt.step() # update critic parameters\n",
        "\n",
        "    crit_losses += [mean_crit_loss] # log critic loss\n",
        "\n",
        "\n",
        "    ### generator ###\n",
        "    gen_opt.zero_grad() # reset gradients\n",
        "\n",
        "    noise = gen_noise(cur_batch_size, z_dim) # generate random noise\n",
        "    fake = gen(noise) # generate fake images\n",
        "    crit_fake_pred = crit(fake) # critic prediction on generated images\n",
        "\n",
        "    gen_loss = -crit_fake_pred.mean() # generator aims to maximise critic's score\n",
        "    gen_loss.backward() # backpropogate\n",
        "    gen_opt.step() # update generator parameters\n",
        "\n",
        "    gen_losses += [gen_loss.item()] # log generator loss\n",
        "\n",
        "\n",
        "    ### stats ###\n",
        "\n",
        "    if (wandbact == 1): # log to Weights & Biases if active\n",
        "      wandb.log({\n",
        "          'epoch': epoch,\n",
        "          'step': cur_step,\n",
        "          'generator loss': gen_loss,\n",
        "          'critic loss': mean_crit_loss\n",
        "      })\n",
        "\n",
        "    if (cur_step % save_step == 0 and cur_step > 0): # save model periodically\n",
        "      print(\"Saving checkpoint:\", cur_step, save_step)\n",
        "      save_checkpoint(\"latest\")\n",
        "\n",
        "    if (cur_step % show_step == 0 and cur_step > 0): # show generated and real images periodically\n",
        "      show(fake, wandbactive = 1, name = 'fake') # visualise fake images\n",
        "      show(real, wandbactive = 1, name = 'real') # visualise real images\n",
        "\n",
        "      gen_mean = sum(gen_losses[-show_step:]) / show_step # average generator loss over window\n",
        "      crit_mean = sum(crit_losses[-show_step:]) / show_step # average critic loss over window\n",
        "      print(f\"Epoch {epoch}, step {cur_step}: Generator loss: {gen_mean}, Critic loss: {crit_mean}\")\n",
        "\n",
        "      plt.plot( # plot generator loss\n",
        "        range(len(gen_losses)),\n",
        "        torch.Tensor(gen_losses),\n",
        "        label = (\"Generator Loss\"),\n",
        "        color = 'blue'\n",
        "      )\n",
        "\n",
        "      plt.plot( # plot critic loss\n",
        "        range(len(crit_losses)),\n",
        "        torch.Tensor(crit_losses),\n",
        "        label = (\"Critic Loss\"),\n",
        "        color = 'red'\n",
        "      )\n",
        "\n",
        "      plt.ylim(-1000, 1000) # set y-axis limits\n",
        "      plt.legend() # show legend\n",
        "      plt.show() # display plot\n",
        "\n",
        "    cur_step += 1 # increment training step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cX6V00UnCGw"
      },
      "outputs": [],
      "source": [
        "# number of steps per epoch\n",
        "# 10000 / 128 = 78.125\n",
        "# 50000 / 128 = 390.625"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PG4PkoLnrD3e"
      },
      "outputs": [],
      "source": [
        "#### Generate new faces\n",
        "noise = gen_noise(batch_size, z_dim)\n",
        "fake = gen(noise)\n",
        "show(fake)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ewo0OEJrG37"
      },
      "outputs": [],
      "source": [
        "plt.imshow(fake[22].detach().cpu().permute(1,2,0).squeeze().clip(0,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyRd9cXtrIIM"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "\n",
        "# MORPHING, interpolation between points in latent space\n",
        "gen_set=[]\n",
        "z_shape=[1,200,1,1]\n",
        "rows=4\n",
        "steps=17\n",
        "\n",
        "for i in range(rows):\n",
        "  z1,z2 = torch.randn(z_shape), torch.randn(z_shape)\n",
        "  for alpha in np.linspace(0,1,steps):\n",
        "    z=alpha*z1 + (1-alpha)*z2\n",
        "    res=gen(z.cuda())[0]\n",
        "    gen_set.append(res)\n",
        "\n",
        "fig = plt.figure(figsize=(25,11))\n",
        "grid=ImageGrid(fig, 111, nrows_ncols=(rows,steps), axes_pad=0.1)\n",
        "\n",
        "for ax , img in zip (grid, gen_set):\n",
        "  ax.axis('off')\n",
        "  res=img.cpu().detach().permute(1,2,0)\n",
        "  res=res-res.min()\n",
        "  res=res/(res.max()-res.min())\n",
        "  ax.imshow(res.clip(0,1.0))\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}